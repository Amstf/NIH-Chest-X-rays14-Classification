{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load in PyTorch's pretrained network\nimport numpy as np\nimport os\nimport torchvision.models as models\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nimport torch.nn.functional as func\nimport pandas as pd\nfrom torch.utils.data.dataset import Dataset\nfrom PIL import Image\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom random import shuffle\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as sk_metrics\nimport warnings\nfrom sklearn.exceptions import UndefinedMetricWarning\nfrom sklearn.metrics import classification_report\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-13T13:38:47.746966Z","iopub.execute_input":"2021-10-13T13:38:47.747227Z","iopub.status.idle":"2021-10-13T13:38:47.754555Z","shell.execute_reply.started":"2021-10-13T13:38:47.747198Z","shell.execute_reply":"2021-10-13T13:38:47.753485Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\nclass chxrayData(Dataset):\n    def __init__(self, dataframe,is_trainset):\n        self.dataframe = dataframe\n        classes = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia',\n                   'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax',\n                   'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema',\n                   'Consolidation']\n\n        self.labels = np.asarray(self.dataframe.loc[:,classes])\n        self.img_names = self.dataframe['Image index']\n       \n\n        # Normalization and data augmentation\n        self.train_transforms = transforms.Compose([\n            transforms.Resize(270),\n            transforms.RandomHorizontalFlip(),\n            transforms.RandomRotation(10),\n            transforms.CenterCrop(256),\n            transforms.ToTensor(),\n            # Normalize by using pre-computed dataset statistics\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        self.test_transforms = transforms.Compose([\n            transforms.Resize(256),\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n        ])\n        self.is_train = is_trainset\n        \n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        \n        imagePath=self.img_names[index]\n        if self.is_train:\n            image = self.train_transforms(Image.open(imagePath).convert('RGB'))\n            \n        else:\n            image = self.test_transforms(Image.open(imagePath).convert('RGB'))\n        \n        label = self.labels[index]\n        \n        return (image, label)\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-13T13:39:24.106580Z","iopub.execute_input":"2021-10-13T13:39:24.106963Z","iopub.status.idle":"2021-10-13T13:39:24.120999Z","shell.execute_reply.started":"2021-10-13T13:39:24.106920Z","shell.execute_reply":"2021-10-13T13:39:24.120338Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"writer = SummaryWriter()\n\ntest_df=pd.read_csv('../input/data1123/test_list.csv')\ntrain_df=pd.read_csv('../input/data1123/train_list.csv')\nval_df=pd.read_csv('../input/data1123/val_list.csv')\n# Create dataset and data loader\ntest_dataset =  chxrayData(test_df, False)\ntrain_dataset =  chxrayData(train_df, True)\nval_dataset =   chxrayData(val_df, False)\n\n# More num_workers consumes more memory for good for speeding up I/O\n# pin_memory=True enables fast data transfer to GPUs Source :https://www.kaggle.com/c/understanding_cloud_organization/discussion/112582\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, pin_memory=True, num_workers=8)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=8)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=True, pin_memory=True, num_workers=8)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:37.354076Z","iopub.execute_input":"2021-10-13T13:39:37.354360Z","iopub.status.idle":"2021-10-13T13:39:41.791941Z","shell.execute_reply.started":"2021-10-13T13:39:37.354330Z","shell.execute_reply":"2021-10-13T13:39:41.791161Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Source: https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, gamma=1.0, alpha=0.25):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, model_output, target):\n        BCE_loss = func.binary_cross_entropy_with_logits(model_output, target)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n        return F_loss","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:44.205010Z","iopub.execute_input":"2021-10-13T13:39:44.205857Z","iopub.status.idle":"2021-10-13T13:39:44.212969Z","shell.execute_reply.started":"2021-10-13T13:39:44.205802Z","shell.execute_reply":"2021-10-13T13:39:44.211944Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"\nclass CHXModel():\n    \n    \n    def __init__(self):\n        self.model = None\n        self.losses = None\n        self.optimizer = None\n        self.model_losses = None\n    # Freeze or un-freeze model layers as required\n    def update_grad(self, grad_val):\n        for param in self.model.parameters():\n            param.requires_grad = grad_val\n    \n     # Initial model setup\n    def set_up_model(self, n_classes, lr, unfreeze, pretrained, use_focal, gamma=1.0, alpha=0.25):\n        self.model = models.resnet34(pretrained=pretrained, progress=True)\n        # Freeze all layers\n        self.update_grad(unfreeze)\n        # Resnet has one fully connected layer, which outputs dimensions of n_classes\n        num_ftrs = self.model.fc.in_features\n        self.model.fc = torch.nn.Linear(num_ftrs, n_classes)\n        self.model.cuda()\n        \n        ''' Use a binary cross-entropy loss function; Applies sigmoid internally (generating probabilities)\n        On the probabilities, cross-entropy loss is computed\n        Because we are doing multilabel, this is better as the value outtputted (unlike softmax)\n        is independent of the other values (while in softmax, probabilities must add up to one)\n        '''\n            \n        if use_focal:\n            self.losses = FocalLoss(gamma, alpha)\n            print(f'Using Focal Loss: {gamma}, {alpha}')\n        else:\n            # Regular binary cross entropy\n            self.losses = torch.nn.BCEWithLogitsLoss()\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n        \n      \n       \n        \n    def load_checkpoint(self, file_path):\n        checkpoint = torch.load(file_path)\n        self.model.load_state_dict(checkpoint['state'])\n        self.optimizer.load_state_dict(checkpoint['optimizer'])\n        return checkpoint['epoch']\n\n    def save_checkpoint(self, state, filename):\n        torch.save(state, filename)      \n    \n    def evaluate(self, data_loader, is_validation=True):\n        \n      \n        average_loss = 0\n        average_auc = 0\n        \n        \n        with torch.no_grad():\n            # Go through each batch in the testloader\n            for X, y in data_loader:\n                input_img = X.cuda(non_blocking=True)\n                labels = y.float().cuda(non_blocking=True)\n                self.optimizer.zero_grad()\n                output = self.model(input_img)\n                sig = torch.nn.Sigmoid()\n                probabilities = sig(output)\n                predictions = probabilities >= 0.30\n                # Push data to CPU first\n                y = y.cpu()\n                predictions = np.array(predictions.cpu()).astype(int)\n                probabilities = np.array(probabilities.cpu())\n                for i in range(len(y)):\n                    if sum(predictions[i,:]) == 0:\n                            max_idx = np.where(probabilities[i,:]==max(probabilities[i,:]))\n                            predictions[i, max_idx] = 1\n                    cur_loss = sk_metrics.hamming_loss(y[i,:], predictions[i,:])\n                    auc = sk_metrics.roc_auc_score(y[i,:], probabilities[i,:], average='micro')\n          \n                   \n                    \n                    \n                    average_loss += cur_loss\n                    average_auc += auc\n                   \n                    \n        if is_validation:\n            total = len(val_dataset)\n        else:\n            total = len(test_dataset)\n        average_loss = average_loss/total\n        average_auc = average_auc/total\n        return average_auc, average_loss    \n    \n        \n    def train(self, epochs, trainloader, val_loader=None, checkpoint_path=None, root_path=None):\n        # Empty cache before training\n        torch.cuda.empty_cache()\n        curr_auc = 1000\n        if checkpoint_path != None:\n            start_epoch = self.load_checkpoint(checkpoint_path)\n        else:\n            start_epoch = 0\n        for e in range(start_epoch, epochs):\n            # Get loss per epoch\n            running_loss = 0\n            for i, (X, y) in enumerate(trainloader):\n                input_img = X.cuda(non_blocking=True)\n                labels = y.float().cuda(non_blocking=True)\n                self.optimizer.zero_grad()\n                output = self.model(input_img)\n                loss = self.losses(output, labels)\n                loss.backward()\n                self.optimizer.step()\n                running_loss = running_loss + loss.item()\n            \n            # Stop training if validation auc score is now dropping\n            if val_loader != None:\n                average_auc, average_loss = self.evaluate(val_loader)\n                if average_auc > curr_auc:\n                    break\n            \n            # Find the loss for the current epoch\n            loss = running_loss/len(trainloader)\n            print(f\"Epoch {e + 1} - Loss: {loss}\")\n          \n            state = {\n                'epoch': e + 1,\n                'state': self.model.state_dict(),\n                'optimizer': self.optimizer.state_dict()\n            }\n            if root_path == None:\n                root_path = './checkpoint_epoch_'\n            writer.add_scalar(\"Loss/train\", loss, epochs)\n            checkpoint_path = os.path.join(root_path + str(e + 1) + '.pth.tar') \n            self.save_checkpoint(state, checkpoint_path)    ","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:46.391610Z","iopub.execute_input":"2021-10-13T13:39:46.392049Z","iopub.status.idle":"2021-10-13T13:39:46.431166Z","shell.execute_reply.started":"2021-10-13T13:39:46.392009Z","shell.execute_reply":"2021-10-13T13:39:46.430370Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nclasses = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia',\n       'Infiltration', 'Mass', 'Nodule', 'Atelectasis', 'Pneumothorax',\n       'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Edema',\n       'Consolidation']\ndef plot_images(img, predictions, correctlabel):\n        plt.figure(num=None, figsize=(8, 6))\n        plt.imshow(img[0,:,:], cmap='gray')\n        correct_labels = [classes[idx] for (idx, val) in enumerate(correctlabel) if val == 1]\n        \n        preds = [classes[idx] for (idx, val) in enumerate(predictions) if val == True]\n        correct_labels = \", \".join(correct_labels)\n        preds = \", \".join(preds)\n        print(f'Correct Labels: {correct_labels}')\n        print(f'Predicted Labels: {preds}')\n        plt.show()\n\n\nto_plot=70\ndef get_images(cur_model):\n    cor_count = 0\n    incor_count = 0\n    with torch.no_grad():\n        # Go through each batch in the testloader\n        for X, y in test_loader:\n            input_img = X.cuda(non_blocking=True)\n            labels = y.float().cuda(non_blocking=True)\n            cur_model.optimizer.zero_grad()\n            output = cur_model.model(input_img)\n            sig = torch.nn.Sigmoid()\n            probabilities = sig(output)\n            predictions = probabilities >= 0.30\n            y = y.cpu()\n            predictions = np.array(predictions.cpu()).astype(int)\n            probabilities = np.array(probabilities.cpu())\n            for i in range(len(y)): \n                if sum(predictions[i,:]) == 0:\n                    max_idx = np.where(probabilities[i,:]==max(probabilities[i,:]))\n                    predictions[i, max_idx] = 1\n                    \n                cur_loss = sk_metrics.hamming_loss(y[i,:], predictions[i,:])\n                if cur_loss == 0.0 and cor_count < to_plot:\n                    plot_images(input_img[i].cpu(), predictions[i,:], y[i,:])\n                    cor_count += 1\n                elif cur_loss > 0.0 and incor_count < to_plot:\n                    plot_images(input_img[i].cpu(), predictions[i,:], y[i,:])\n                    incor_count += 1\n        \n        \n       \n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:39:52.672498Z","iopub.execute_input":"2021-10-13T13:39:52.673102Z","iopub.status.idle":"2021-10-13T13:39:52.687215Z","shell.execute_reply.started":"2021-10-13T13:39:52.673062Z","shell.execute_reply":"2021-10-13T13:39:52.686001Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_classes = 14\nlearning_rate = 0.0005\nchx_model_focal = CHXModel()\nchx_model_focal.set_up_model(n_classes, learning_rate, unfreeze=True, pretrained=True, use_focal=True, gamma=2.5, alpha=0.5)\npath_to_model = '../input/checkpoint115/checkpoint_epoch_15.pth.tar'\nchx_model_focal.train(15, train_loader, root_path='./checkpoint_epoch_', checkpoint_path=path_to_model)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:40:01.777190Z","iopub.execute_input":"2021-10-13T13:40:01.777500Z","iopub.status.idle":"2021-10-13T13:40:15.422066Z","shell.execute_reply.started":"2021-10-13T13:40:01.777467Z","shell.execute_reply":"2021-10-13T13:40:15.421263Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model_auc, model_loss= chx_model_focal.evaluate(test_loader, is_validation=False)\nprint(f'Average AUC: {model_auc}')\nprint(f'Average Hamming Loss: {model_loss}')","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:40:16.475625Z","iopub.execute_input":"2021-10-13T13:40:16.476432Z","iopub.status.idle":"2021-10-13T13:41:16.681540Z","shell.execute_reply.started":"2021-10-13T13:40:16.476359Z","shell.execute_reply":"2021-10-13T13:41:16.679797Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"get_images(chx_model_focal)","metadata":{"execution":{"iopub.status.busy":"2021-10-13T13:41:25.833225Z","iopub.execute_input":"2021-10-13T13:41:25.833721Z","iopub.status.idle":"2021-10-13T13:42:49.096492Z","shell.execute_reply.started":"2021-10-13T13:41:25.833680Z","shell.execute_reply":"2021-10-13T13:42:49.095686Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a href=\"./checkpoint_epoch_15.pth.tar\"> Download File </a>","metadata":{}}]}